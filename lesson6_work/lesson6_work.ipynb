{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\py36\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import os\n",
    "import pyltp\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 返回分句后的list\n",
    "def sentence_split(string):\n",
    "    sentences = list(pyltp.SentenceSplitter.split(string))\n",
    "    return [e for e in sentences if e]\n",
    "\n",
    "# list整合并存入文件\n",
    "def list2file(input_list, output_file):\n",
    "    output_file.writelines(' '.join(input_list) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 加载新闻语料\n",
    "news_data = pd.read_csv(\n",
    "    '..\\\\data\\\\export_sql_1558435\\\\sqlResult_1558435.csv', encoding='gb18030')\n",
    "# 空值处理\n",
    "news_data = news_data.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分句存入文件\n",
    "with open(\n",
    "        '..\\\\data\\\\export_sql_1558435\\\\sentences_1558435.txt',\n",
    "        'w',\n",
    "        encoding='utf8') as fi:\n",
    "    for i in news_data.index:\n",
    "        content = news_data.at[i, 'content']\n",
    "        list2file(sentence_split(content), fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LTP_DATA_DIR = '../ltp_data'  # ltp模型目录的路径\n",
    "cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')\n",
    "segmentor = pyltp.Segmentor()\n",
    "segmentor.load(cws_model_path)\n",
    "\n",
    "def cut(string):\n",
    "    words = segmentor.segment(string)\n",
    "    return list(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = cut('孔子说，学习了而时常温习，不也喜悦吗？')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词存入文件\n",
    "with open(\n",
    "        '..\\\\data\\\\export_sql_1558435\\\\corpus_1558435.txt', 'w',\n",
    "        encoding='utf8') as fi:\n",
    "    for i in news_data.index:\n",
    "        content = news_data.at[i, 'content']\n",
    "        list2file(cut(content), fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词性标注模型\n",
    "pos_model_path = os.path.join(LTP_DATA_DIR, 'pos.model')\n",
    "postagger = pyltp.Postagger()\n",
    "postagger.load(pos_model_path)\n",
    "\n",
    "def words_tagging(words):\n",
    "    return list(postagger.postag(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "postags = words_tagging(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 命名实体识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model_path = os.path.join(LTP_DATA_DIR, 'ner.model')\n",
    "recognizer = pyltp.NamedEntityRecognizer()\n",
    "recognizer.load(ner_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(words, postags):\n",
    "    return list(recognizer.recognize(words, postags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "netags = ner(words, postags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S-Nh', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 句法分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对句子进行依存句法分析\n",
    "parser_model_path = os.path.join(LTP_DATA_DIR, 'parser.model')\n",
    "parser = pyltp.Parser()\n",
    "parser.load(parser_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 做语法分析，分析结果以列表形式返回\n",
    "def parse(words, postags):\n",
    "    arcs = parser.parse(words,postags)\n",
    "    return [(arc.head,arc.relation) for arc in arcs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcs = parse(words, postags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 'SBV'),\n",
       " (0, 'HED'),\n",
       " (2, 'WP'),\n",
       " (2, 'VOB'),\n",
       " (4, 'RAD'),\n",
       " (8, 'ADV'),\n",
       " (8, 'ADV'),\n",
       " (4, 'COO'),\n",
       " (4, 'WP'),\n",
       " (12, 'ADV'),\n",
       " (12, 'ADV'),\n",
       " (4, 'COO'),\n",
       " (12, 'RAD'),\n",
       " (2, 'WP')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练表达观点的关键词模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 根据新闻语料训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for i in news_data.index:\n",
    "    for sentence in sentence_split(news_data.at[i,'content']):\n",
    "        sentences.append(cut(sentence))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练word2vec并导出\n",
    "news_word2vec_model = Word2Vec(sentences, min_count=5, size=50)\n",
    "\n",
    "# news_word2vec_model.save('news_word2vec_model')\n",
    "# 模型的导入\n",
    "# news_word2vec_model = Word2Vec.load('news_word2vec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_word2vec_model = Word2Vec.load('news_word2vec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\py36\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('表示', 0.866018533706665),\n",
       " ('坦言', 0.8141570687294006),\n",
       " ('指出', 0.809965968132019),\n",
       " ('认为', 0.8057894706726074),\n",
       " ('告诉', 0.7455646991729736),\n",
       " ('称', 0.7256631255149841),\n",
       " ('强调', 0.7111589908599854),\n",
       " ('看来', 0.6905254125595093),\n",
       " ('透露', 0.6809438467025757),\n",
       " ('写道', 0.6728281378746033)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_word2vec_model.wv.most_similar('说')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结合wikipedia模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入wikipedia的模型\n",
    "wiki_word2vec_model = Word2Vec.load('..\\\\lesson5\\\\word2vec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_search_tune(init_word, model, limit_rate=0.7):\n",
    "    max_size = 1000\n",
    "    seen = defaultdict(int)\n",
    "    need_seen = [init_word]\n",
    "    n = 0\n",
    "\n",
    "    while need_seen and len(seen) < max_size:\n",
    "        if n % 1000 == 0:\n",
    "            print('had run counts:{}'.format(n))\n",
    "        if n > 5000:\n",
    "            break\n",
    "        node = need_seen.pop(0)\n",
    "        new_words = [\n",
    "            w for w, p in model.wv.most_similar(node, topn=15)\n",
    "            if p > limit_rate\n",
    "        ] \n",
    "        need_seen += new_words\n",
    "        seen[node] += 1\n",
    "        n += 1\n",
    "\n",
    "    return seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过加权的方式合并\n",
    "def merge_model(news_rate,wiki_rate,topn=50):\n",
    "    combine_similar = defaultdict(int)\n",
    "\n",
    "    news_update_similar = graph_search_tune('说', news_word2vec_model)\n",
    "    # 由于wiki中词汇较多，因此对概率限制更严格\n",
    "    wiki_update_similar = graph_search_tune('说', wiki_word2vec_model, 0.83)\n",
    "    \n",
    "    for key in news_update_similar:\n",
    "        combine_similar[key] = news_update_similar[\n",
    "            key] * news_rate + wiki_update_similar[key] * wiki_rate\n",
    "        \n",
    "    result = sorted(combine_similar.items(), key=lambda x: x[1], reverse=True)\n",
    "    return result[:topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "had run counts:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\py36\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "had run counts:1000\n",
      "had run counts:2000\n",
      "had run counts:3000\n",
      "had run counts:4000\n",
      "had run counts:5000\n",
      "had run counts:0\n",
      "had run counts:1000\n",
      "had run counts:2000\n",
      "had run counts:3000\n",
      "had run counts:4000\n",
      "had run counts:5000\n"
     ]
    }
   ],
   "source": [
    "similar_top50 = [i for i, j in merge_model(0.5,0.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 观点提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取主语索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1、主语：SBV\n",
    "2、可能存在代词的情况。需要判断是否为人名，机构名\n",
    "'''\n",
    "# 获取主语索引\n",
    "def get_sbv_idx(words, netags, arcs, key_word_idx):\n",
    "    n = 0\n",
    "    for i in arcs:\n",
    "        is_person_or_org = 'Nh' in netags[n] or 'Ni' in netags[n]\n",
    "        if i[0] == key_word_idx + 1 and i[1] == 'SBV' and is_person_or_org:\n",
    "            return n\n",
    "        n += 1\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取观点start和end索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 判断句子中核心动词的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n1、如果存在多个与'说'相关的关键词，其中存在有'HED'标记的，取'HED'的动词为关键词\\n2、如果'HED'与'说'无关，但与'说'相关的关键词与'HED'为'COO'关系。则判断各个关键词的影响，选取在句中影响最大的关键词\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1、如果存在多个与'说'相关的关键词，其中存在有'HED'标记的，取'HED'的动词为关键词\n",
    "2、如果'HED'与'说'无关，但与'说'相关的关键词与'HED'为'COO'关系。则判断各个关键词的影响，选取在句中影响最大的关键词\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keyword_idx(words, arcs):\n",
    "    current_idx = 0\n",
    "    importance = []\n",
    "    for arc in arcs:\n",
    "        if arc[1] == 'HED':\n",
    "            if words[current_idx] in similar_top50:\n",
    "                return current_idx\n",
    "            # 存下hed的下标\n",
    "            hed_idx = current_idx\n",
    "        elif arc[1] == 'COO' and words[current_idx] in similar_top50:\n",
    "            count = 0\n",
    "            for i in arcs:\n",
    "                if i[0] == current_idx+1:\n",
    "                    count += 1\n",
    "            importance.append((current_idx, count))\n",
    "        current_idx += 1\n",
    "        \n",
    "#   找到了关键词\n",
    "    if importance:\n",
    "        most_important_idx = sorted(importance,key=lambda x:x[1],reverse=True)[0][0]\n",
    "        return most_important_idx\n",
    "#   没有找到关键词\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 判断宾语在动词的前后位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vob_after_hed(arcs,key_word_idx):\n",
    "    current_idx = key_word_idx\n",
    "    for arc in arcs[key_word_idx:]:\n",
    "        if arc[0] == key_word_idx+1 and arc[1]=='VOB':\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 宾语在动词之前，提取观点分割符号位置索引\n",
    "def before_wp_idx(words, arcs, key_word_idx):\n",
    "    current_idx = 0\n",
    "    start_index = 0\n",
    "    end_index = 0\n",
    "    for arc in arcs[:key_word_idx]:\n",
    "        if arc[0] == key_word_idx + 1 and arc[1] == 'WP':\n",
    "            if start_index == 0:\n",
    "                start_index = current_idx\n",
    "            else:\n",
    "                end_index = current_idx\n",
    "        current_idx += 1\n",
    "    return start_index, end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 宾语在动词之后，获取观点分割符号位置索引\n",
    "def after_wp_idx(words, arcs, key_word_idx):\n",
    "    current_idx = key_word_idx\n",
    "    start_index = 0\n",
    "    end_index = 0\n",
    "    for arc in arcs[key_word_idx:]:\n",
    "        if arc[0] == key_word_idx + 1 and arc[1] == 'WP':\n",
    "            if start_index == 0:\n",
    "                start_index = current_idx\n",
    "            else:\n",
    "                end_index = current_idx\n",
    "        current_idx += 1\n",
    "    return start_index, end_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据动宾关系的前后获取观点位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wp_idx(words, arcs, key_word_idx):\n",
    "    if vob_after_hed(arcs, key_word_idx):\n",
    "        return after_wp_idx(words, arcs, key_word_idx)\n",
    "    else:\n",
    "        return before_wp_idx(words, arcs, key_word_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提取观点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对含有说的句子解析并提取观点\n",
    "# input:分词与结构标记，关键词列表\n",
    "def get_view(words, netags, arcs, key_word_idx):\n",
    "    \n",
    "    start_idx, end_idx = get_wp_idx(words, arcs, key_word_idx)\n",
    "    sbv_idx = get_sbv_idx(words, netags, arcs, key_word_idx)\n",
    "    if sbv_idx!=None:\n",
    "        return  words[sbv_idx],''.join(words[start_idx + 1:end_idx + 1])\n",
    "#     如果没有找到人名或机构名，主语返回None\n",
    "    return  None,''.join(words[start_idx + 1:end_idx + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content2view(content):\n",
    "    collections = []\n",
    "    views = []\n",
    "    # 分句\n",
    "    sentences = sentence_split(content)\n",
    "    # 对句子进行分词与词性标注，\n",
    "    for sentence in sentences:\n",
    "        words = cut(sentence)\n",
    "        postags = words_tagging(words)\n",
    "        netags = ner(words, postags)\n",
    "        arcs = parse(words, postags)\n",
    "\n",
    "        #         如果分词含有‘说’的近似词\n",
    "        if set(words)&set(similar_top50):\n",
    "            key_word_idx = get_keyword_idx(words, arcs)\n",
    "            if key_word_idx != None:\n",
    "                views.append(get_view(words, netags, arcs, key_word_idx))\n",
    "    return views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('孔子', '学习了而时常温习，不也喜悦吗？')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''孔子他说，学习了而时常温习，不也喜悦吗？有朋友从远方来说话，不也快乐吗？别人不理解自己也不怨恨，不也是君子吗？'''\n",
    "content2view(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取新闻语料观点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "had run 0 times\n",
      "had run 100 times\n",
      "had run 200 times\n",
      "had run 300 times\n",
      "had run 400 times\n",
      "had run 500 times\n",
      "had run 600 times\n",
      "had run 700 times\n",
      "had run 800 times\n",
      "had run 900 times\n",
      "had run 1000 times\n",
      "had run 1100 times\n",
      "had run 1200 times\n",
      "had run 1300 times\n",
      "had run 1400 times\n",
      "had run 1500 times\n",
      "had run 1600 times\n",
      "had run 1700 times\n",
      "had run 1800 times\n",
      "had run 1900 times\n",
      "had run 2000 times\n",
      "had run 2100 times\n",
      "had run 2200 times\n",
      "had run 2300 times\n",
      "had run 2400 times\n"
     ]
    }
   ],
   "source": [
    "# 分句存入文件\n",
    "with open(\n",
    "        'view_data.txt',\n",
    "        'w',\n",
    "        encoding='utf8') as fi:\n",
    "    for i in news_data.index:\n",
    "        if i%500==0:\n",
    "            print('had run {} times'.format(i))\n",
    "        content = news_data.at[i, 'content']\n",
    "        fi.writelines(str(view)+'\\n' for view in content2view(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
